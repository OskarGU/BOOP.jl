<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>References · BOOP.jl</title><meta name="title" content="References · BOOP.jl"/><meta property="og:title" content="References · BOOP.jl"/><meta property="twitter:title" content="References · BOOP.jl"/><meta name="description" content="Documentation for BOOP.jl."/><meta property="og:description" content="Documentation for BOOP.jl."/><meta property="twitter:description" content="Documentation for BOOP.jl."/><meta property="og:url" content="https://OskarGU.github.io/BOOP.jl/reference/"/><meta property="twitter:url" content="https://OskarGU.github.io/BOOP.jl/reference/"/><link rel="canonical" href="https://OskarGU.github.io/BOOP.jl/reference/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">BOOP.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>References</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Ramaining-functions"><span>Ramaining functions</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>References</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>References</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/OskarGU/BOOP.jl/blob/main/docs/src/reference.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><p>continue docs just to see order</p><h1 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h1><ul><li><a href="#BOOP.BO-NTuple{4, Any}"><code>BOOP.BO</code></a></li><li><a href="#BOOP.ExpectedMaxGaussian-Tuple{Vector{Float64}, Vector{Float64}}"><code>BOOP.ExpectedMaxGaussian</code></a></li><li><a href="#BOOP.estimate_integral_wsabi-Tuple{Any, Any}"><code>BOOP.estimate_integral_wsabi</code></a></li><li><a href="#BOOP.expected_improvement-Tuple{Any, Any, Any}"><code>BOOP.expected_improvement</code></a></li><li><a href="#BOOP.expected_improvement_boundary-Tuple{Any, Any, Any}"><code>BOOP.expected_improvement_boundary</code></a></li><li><a href="#BOOP.first_func-Tuple{Any}"><code>BOOP.first_func</code></a></li><li><a href="#BOOP.inv_rescale-Tuple{Any, Any, Any}"><code>BOOP.inv_rescale</code></a></li><li><a href="#BOOP.knowledgeGradientDiscrete-Tuple{Any, Any, Any}"><code>BOOP.knowledgeGradientDiscrete</code></a></li><li><a href="#BOOP.knowledgeGradientHybrid-Tuple{GaussianProcesses.GPE, Any}"><code>BOOP.knowledgeGradientHybrid</code></a></li><li><a href="#BOOP.knowledgeGradientMonteCarlo-Tuple{Any, Any}"><code>BOOP.knowledgeGradientMonteCarlo</code></a></li><li><a href="#BOOP.multi_start_maximize-Tuple{Any, Any, Any}"><code>BOOP.multi_start_maximize</code></a></li><li><a href="#BOOP.multi_start_minimize-Tuple{Any, Any, Any}"><code>BOOP.multi_start_minimize</code></a></li><li><a href="#BOOP.posteriorMax-Tuple{Any}"><code>BOOP.posteriorMax</code></a></li><li><a href="#BOOP.posteriorMaxObs-Tuple{Any, Any}"><code>BOOP.posteriorMaxObs</code></a></li><li><a href="#BOOP.posterior_cov-Tuple{GaussianProcesses.GPE, AbstractMatrix, AbstractMatrix}"><code>BOOP.posterior_cov</code></a></li><li><a href="#BOOP.posterior_variance-Tuple{Any, Any}"><code>BOOP.posterior_variance</code></a></li><li><a href="#BOOP.propose_next-Tuple{Any, Any}"><code>BOOP.propose_next</code></a></li><li><a href="#BOOP.rescale-Tuple{Any, Any, Any}"><code>BOOP.rescale</code></a></li><li><a href="#BOOP.upper_confidence_bound-Tuple{Any, Any}"><code>BOOP.upper_confidence_bound</code></a></li></ul><h1 id="Ramaining-functions"><a class="docs-heading-anchor" href="#Ramaining-functions">Ramaining functions</a><a id="Ramaining-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Ramaining-functions" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.BO-NTuple{4, Any}" href="#BOOP.BO-NTuple{4, Any}"><code>BOOP.BO</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BO(f, modelSettings, optimizationSettings, warmStart)</code></pre><p>Performs a full Bayesian Optimization run. ... (resten av docstringen) ...</p><p><strong>Returns</strong></p><ul><li>A <code>Tuple</code> containing:<ul><li><code>gp</code>: The final, trained GP model.</li><li><code>X</code>: The full <code>n x d</code> matrix of all evaluated points.</li><li><code>y</code>: The full <code>n</code>-element vector of all observations.</li><li><code>objectMaximizer</code>: The location <code>x</code> of the global maximum of the final posterior mean.</li><li><code>objectMaximizerY</code>: The predicted posterior mean value at <code>objectMaximizer</code>. # &lt;– NEW</li><li><code>postMaxObserved</code>: The location <code>x</code> of the observed point with the highest posterior mean.</li><li><code>postMaxObservedY</code>: The predicted posterior mean value at <code>postMaxObserved</code>.</li></ul></li></ul><p>...</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/bayesoptfunctions.jl#L156-L172">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.ExpectedMaxGaussian-Tuple{Vector{Float64}, Vector{Float64}}" href="#BOOP.ExpectedMaxGaussian-Tuple{Vector{Float64}, Vector{Float64}}"><code>BOOP.ExpectedMaxGaussian</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ExpectedMaxGaussian(μ::Vector{Float64}, σ::Vector{Float64})</code></pre><p>Analytically computes the expectation <code>E[max(μ + σZ)]</code> where <code>Z ~ N(0,1)</code>.</p><p>This is the core for <code>knowledgeGradientDiscrete</code>. It calculates the expected maximum of a set of correlated Gaussian random variables that share a single source of randomness, <code>Z</code>. The function is robust to cases where slopes (<code>σ</code>) are equal or nearly equal.</p><p><strong>Arguments</strong></p><ul><li><code>μ::Vector{Float64}</code>: A vector of the current means of the random variables.</li><li><code>σ::Vector{Float64}</code>: A vector of the sensitivities (&quot;slopes&quot;) of each random variable with respect to the common random factor <code>Z</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The analytically computed expected maximum value.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L251-L268">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.estimate_integral_wsabi-Tuple{Any, Any}" href="#BOOP.estimate_integral_wsabi-Tuple{Any, Any}"><code>BOOP.estimate_integral_wsabi</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">estimate_integral_wsabi(gp, bounds; n_samples=100_000, y_mean=0.0, y_std=1.0)</code></pre><p>Estimates the integral of the original function f(x) using the final GP model trained on warped data g(x) = log(f(x)).</p><p>It uses Monte Carlo integration on the posterior expectation of f(x). E[f(x)] = exp(μ<em>g(x) + σ²</em>g(x)/2), where μ<em>g and σ²</em>g are the posterior mean and variance of the GP fitted to the log-transformed data.</p><p><strong>Arguments</strong></p><ul><li><code>gp</code>: The final trained GP model.</li><li><code>bounds</code>: A tuple (lo, hi) defining the integration domain.</li><li><code>n_samples</code>: Number of Monte Carlo samples for the approximation.</li><li><code>y_mean</code>, <code>y_std</code>: The mean and std dev used to standardize the warped y-values,                    needed to un-scale the GP&#39;s predictions.</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The estimated value of the integral.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L668-L687">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.expected_improvement-Tuple{Any, Any, Any}" href="#BOOP.expected_improvement-Tuple{Any, Any, Any}"><code>BOOP.expected_improvement</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">  expected_improvement(gp, xnew, fMax; ξ = 0.01)</code></pre><p>Computes the expected improvement given a Gaussian process (<code>gp</code>) object, and the earliest best evaluation (<code>fMax</code>) at the new evaluation point <code>xnew</code>). ξ is a tuning parameter that controls the exploration-exploitation trade-off. A large value of ξ encourages exploration and vice versa.</p><p>Returns the expected improvement at <code>xnew</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Set up GP model.
julia&gt; X_train = [1.0, 2.5, 4.0]; julia&gt; y_train = [sin(x) for x in X_train];
julia&gt; gp_model = GP(X_train&#39;, y_train, MeanZero(), SE(0.0, 0.0));
julia&gt; optimize!(gp_model);

julia&gt; # 2. Define the best observed value and a candidate point
julia&gt; fMax = maximum(y_train);
julia&gt; x_candidate = 3.0;

julia&gt; # 3. Compute Expected Improvement
julia&gt; ei = expected_improvement(gp_model, x_candidate, fMax; ξ=0.01)
≈ 4.89.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L3-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.expected_improvement_boundary-Tuple{Any, Any, Any}" href="#BOOP.expected_improvement_boundary-Tuple{Any, Any, Any}"><code>BOOP.expected_improvement_boundary</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">expected_improvement_boundary(gp, xnew, ybest; ξ = 0.10, bounds=nothing)</code></pre><p>Computes the Expected Improvement (EI) with an optional penalty for points near the boundary of the search space. VERY EXPERIMENTAL.</p><p>The penalty is a multiplicative weight <code>w = prod(1 - x_norm^2)</code> that smoothly pushes the search away from the edges of the scaled <code>[-1, 1]</code> domain. This can be useful to prevent the optimizer from selecting points at the very edge of the feasible region. The idea comes from practical experience that EI otherwise spent too much time exploring the boundaries.</p><p><strong>Arguments</strong></p><ul><li><code>gp</code>: The trained Gaussian Process model.</li><li><code>xnew</code>: The candidate point at which to evaluate the EI.</li><li><code>ybest</code>: The current best observed value.</li><li><code>ξ</code>: The exploration-exploitation trade-off parameter.</li><li><code>bounds</code>: A tuple <code>(lower, upper)</code> defining the original search space, required for the penalty.</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The (potentially penalized) Expected Improvement value.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L72-L92">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.first_func-Tuple{Any}" href="#BOOP.first_func-Tuple{Any}"><code>BOOP.first_func</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">    first_func(x)</code></pre><p>A first dummy function that returns the log of the square of the input value.</p><pre><code class="language-julia-repl hljs">julia&gt; T = 500; ϕ = [sin.(2π*(1:T)/T) -0.1*ones(T)]; θ = zeros(T); σ = 1; μ = 2;
julia&gt; y = simTvARMA(ϕ, θ, μ, σ);
julia&gt;  tvPeriodogram(y, 25, 15)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/FirstFunc.jl#L4-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.inv_rescale-Tuple{Any, Any, Any}" href="#BOOP.inv_rescale-Tuple{Any, Any, Any}"><code>BOOP.inv_rescale</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">inv_rescale(X_scaled, lo, hi)</code></pre><p>Performs the inverse of <code>rescale</code>, scaling an <code>n x d</code> matrix <code>X_scaled</code> from the hypercube <code>[-1, 1]^d</code> back to the original domain.</p><p><strong>Arguments</strong></p><ul><li><code>X_scaled</code>: An <code>n x d</code> matrix where each row is a point in <code>[-1, 1]^d</code>.</li><li><code>lo</code>: A <code>d</code>-element vector of lower bounds for the original domain.</li><li><code>hi</code>: A <code>d</code>-element vector of upper bounds for the original domain.</li></ul><p><strong>Returns</strong></p><ul><li>The un-scaled <code>n x d</code> matrix in the original domain.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; lo = [0.0, 10.0];
julia&gt; hi = [1.0, 20.0];
julia&gt; X_scaled = [0.0 0.0; -1.0 -1.0];
julia&gt; inv_rescale(X_scaled, lo, hi)
2×2 Matrix{Float64}:
 0.5  15.0
 0.0  10.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/bayesoptfunctions.jl#L125-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.knowledgeGradientDiscrete-Tuple{Any, Any, Any}" href="#BOOP.knowledgeGradientDiscrete-Tuple{Any, Any, Any}"><code>BOOP.knowledgeGradientDiscrete</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">knowledgeGradientDiscrete(gp, xnew, domain_points)</code></pre><p>Computes the Knowledge Gradient (KG) acquisition function where the future maximum is constrained to occur on a fixed, discrete set of points.</p><p>This function is the analytical engine for the <code>knowledgeGradientHybrid</code> heuristic.</p><p><strong>Arguments</strong></p><ul><li><code>gp</code>: The trained Gaussian Process model.</li><li><code>xnew</code>: The candidate point at which to evaluate the KG.</li><li><code>domain_points</code>: A <code>d x M</code> matrix of <code>M</code> discrete points in the scaled <code>[-1, 1]</code> space where the future maximum is sought.</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The discrete Knowledge Gradient value.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L368-L384">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.knowledgeGradientHybrid-Tuple{GaussianProcesses.GPE, Any}" href="#BOOP.knowledgeGradientHybrid-Tuple{GaussianProcesses.GPE, Any}"><code>BOOP.knowledgeGradientHybrid</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">knowledgeGradientHybrid(gp, xnew, lower, upper; n_z=5)</code></pre><p>Calculates the Hybrid Knowledge Gradient (KGh) acquisition function for a candidate point <code>xnew</code>.</p><p>The Hybrid KG combines the strengths of the Monte-Carlo and Discrete KG methods. It uses a small, deterministic set of <code>n_z</code> fantasy scenarios to identify a high-potential set of future maximizers (<code>X_MC</code>), and then uses the fast, analytical Discrete KG algorithm on that small set.</p><p><strong>Arguments</strong></p><ul><li><code>gp::GPE</code>: The trained Gaussian Process model.</li><li><code>xnew</code>: The candidate point at which to evaluate the KG.</li><li><code>lower</code>, <code>upper</code>: The bounds of the search domain for finding the fantasy maximizers.</li><li><code>n_z::Int</code>: The number of deterministic Z-samples to use (default is 5, as in the paper).</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The positive Hybrid Knowledge Gradient value.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L423-L441">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.knowledgeGradientMonteCarlo-Tuple{Any, Any}" href="#BOOP.knowledgeGradientMonteCarlo-Tuple{Any, Any}"><code>BOOP.knowledgeGradientMonteCarlo</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">knowledgeGradientMonteCarlo(gp, xnew, lower, upper; n_samples=20)</code></pre><p>Calculates the Knowledge Gradient (KG) acquisition function for a candidate point <code>xnew</code> using Monte Carlo. This version gives a noisy surface to the optimization landscape due to MC variation. Try the &quot;quadrature methods below&quot;.</p><p>The Knowledge Gradient quantifies the expected increase in the maximum estimated value after sampling at <code>xnew</code>.</p><p><strong>Arguments</strong></p><ul><li><code>gp::GPE</code>: The trained Gaussian Process model.</li><li><code>xnew</code>: The candidate point at which to evaluate the KG.</li><li><code>n_samples::Int</code>: The number of Monte Carlo samples to use.</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The positive Knowledge Gradient value. This function returns the natural KG score, which should be maximized by the optimization loop.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L118-L135">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.multi_start_maximize-Tuple{Any, Any, Any}" href="#BOOP.multi_start_maximize-Tuple{Any, Any, Any}"><code>BOOP.multi_start_maximize</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">multi_start_maximize(f, lower, upper; n_starts=20)</code></pre><p>Finds the global maximum of a function <code>f</code> within a given box <code>[lower, upper]</code> by using multiple starting points. This is a wrapper around <code>multi_start_minimize</code> that simply minimizes the negative of the function.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: The function to maximize.</li><li><code>lower</code>: A vector of lower bounds.</li><li><code>upper</code>: A vector of upper bounds.</li><li><code>n_starts::Int</code>: The number of distinct starting points to use.</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Float64, Vector{Float64}}</code>: A tuple <code>(best_max, best_argmax)</code> containing the maximum value found and the location where it was found.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L226-L242">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.multi_start_minimize-Tuple{Any, Any, Any}" href="#BOOP.multi_start_minimize-Tuple{Any, Any, Any}"><code>BOOP.multi_start_minimize</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">multi_start_minimize(f, lower, upper; n_starts=20)</code></pre><p>Finds the global minimum of a function <code>f</code> within a given box <code>[lower, upper]</code> by using multiple starting points.</p><p>This function uses the L-BFGS optimizer from <code>Optim.jl</code> together with autodiff, starting from <code>n_starts</code> evenly spaced points within the domain to increase the probability of finding a global, rather than local, minimum.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: The function to minimize.</li><li><code>lower</code>: A vector of lower bounds.</li><li><code>upper</code>: A vector of upper bounds.</li><li><code>n_starts::Int</code>: The number of distinct starting points to use.</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Float64, Vector{Float64}}</code>: A tuple <code>(best_min, best_argmin)</code> containing the minimum value found and the location where it was found.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L179-L198">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.posteriorMax-Tuple{Any}" href="#BOOP.posteriorMax-Tuple{Any}"><code>BOOP.posteriorMax</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">posteriorMax(gp; n_starts=20)</code></pre><p>Finds the global maximum of the GP&#39;s posterior mean over the entire continuous, scaled domain <code>[-1, 1]^d</code> using multi-start optimization. This maximum can occur at a previously unobserved point.</p><p><strong>Arguments</strong></p><ul><li><code>gp</code>: The trained Gaussian Process model.</li><li><code>n_starts::Int</code>: The number of restarts for the optimization.</li></ul><p><strong>Returns</strong></p><ul><li><code>NamedTuple{(:fX_max, :X_max)}</code>: A named tuple containing the maximum value of the posterior mean (<code>fX_max</code>) and its location (<code>X_max</code>) in the scaled space.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/bayesoptfunctions.jl#L55-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.posteriorMaxObs-Tuple{Any, Any}" href="#BOOP.posteriorMaxObs-Tuple{Any, Any}"><code>BOOP.posteriorMaxObs</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">posteriorMaxObs(gp, X_scaled)</code></pre><p>Finds the maximum of the GP&#39;s posterior mean evaluated only at the points that have already been observed.</p><p><strong>Arguments</strong></p><ul><li><code>gp</code>: The trained Gaussian Process model.</li><li><code>X_scaled</code>: A <code>d x n</code> matrix of the <code>n</code> locations already observed, in the scaled space.</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The maximum value of the posterior mean among the observed points.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/bayesoptfunctions.jl#L77-L90">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.posterior_cov-Tuple{GaussianProcesses.GPE, AbstractMatrix, AbstractMatrix}" href="#BOOP.posterior_cov-Tuple{GaussianProcesses.GPE, AbstractMatrix, AbstractMatrix}"><code>BOOP.posterior_cov</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">posterior_cov(gp::GPE, X1::AbstractMatrix, X2::AbstractMatrix)</code></pre><p>Computes the posterior covariance matrix <code>k_n(X1, X2)</code> between two sets of points <code>X1</code> and <code>X2</code>, given the GP&#39;s training data.</p><p>The calculation uses the formula <code>k(X1, X2) - k(X1, X) * K_inv * k(X, X2)</code>, using the pre-computed Cholesky factorization of the GP&#39;s kernel matrix for high efficiency.</p><p><strong>Arguments</strong></p><ul><li><code>gp::GPE</code>: The trained Gaussian Process model.</li><li><code>X1::AbstractMatrix</code>: A <code>d x n1</code> matrix of <code>n1</code> points.</li><li><code>X2::AbstractMatrix</code>: A <code>d x n2</code> matrix of <code>n2</code> points.</li></ul><p><strong>Returns</strong></p><ul><li><code>Matrix{Float64}</code>: The <code>n1 x n2</code> posterior covariance matrix.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L326-L343">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.posterior_variance-Tuple{Any, Any}" href="#BOOP.posterior_variance-Tuple{Any, Any}"><code>BOOP.posterior_variance</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">posterior_variance(gp, xnew)</code></pre><p>Computes the posterior variance of the Gaussian Process <code>gp</code> at a new point <code>xnew</code>.</p><p>This acquisition function directs sampling to regions of highest uncertainty in the model. It is the primary acquisition function for standard Bayesian Quadrature methods like WSABI, but can also be used for pure exploration in Bayesian Optimization.</p><p><strong>Arguments</strong></p><ul><li><code>gp</code>: The trained Gaussian Process model.</li><li><code>xnew</code>: The candidate point at which to evaluate the posterior variance.</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The posterior variance at <code>xnew</code>.</li></ul><p><strong>Examples</strong></p><p>```julia-repl julia&gt; X<em>train = [1.0, 5.0]; julia&gt; y</em>train = [sin(x) for x in X<em>train]; julia&gt; gp = GP(X</em>train&#39;, y_train, MeanZero(), SE(0.0, 0.0)); julia&gt; optimize!(gp);</p><p>julia&gt; # The point of highest uncertainty is halfway between the samples julia&gt; x<em>candidate = 3.0; julia&gt; posterior</em>variance(gp, x_candidate) 1.0000010000000002</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L626-L653">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.propose_next-Tuple{Any, Any}" href="#BOOP.propose_next-Tuple{Any, Any}"><code>BOOP.propose_next</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">propose_next(gp::GPE, f_max; n_restarts::Int, acq_config::AcquisitionConfig)</code></pre><p>Optimizes the acquisition function to find the best next point to sample. The optimization is performed in the scaled <code>[-1, 1]^d</code> space.</p><p><strong>Arguments</strong></p><ul><li><code>gp::GPE</code>: The current trained Gaussian Process model.</li><li><code>f_max</code>: The current best observed value (or posterior mean max), used by some acquisition functions like EI.</li><li><code>n_restarts::Int</code>: The number of restarts for the multi-start optimization of the acquisition function.</li><li><code>acq_config::AcquisitionConfig</code>: A struct holding the configuration for the chosen acquisition function (e.g., <code>EIConfig</code>, <code>UCBConfig</code>, <code>KGQConfig</code>).</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{Float64}</code>: The coordinates of the next best point to sample, in the <strong>scaled <code>[-1, 1]^d</code> space</strong>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/bayesoptfunctions.jl#L12-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.rescale-Tuple{Any, Any, Any}" href="#BOOP.rescale-Tuple{Any, Any, Any}"><code>BOOP.rescale</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">rescale(X, lo, hi)</code></pre><p>Scales an <code>n x d</code> matrix <code>X</code> from an original domain to the hypercube <code>[-1, 1]^d</code>.</p><p><strong>Arguments</strong></p><ul><li><code>X</code>: An <code>n x d</code> matrix where each row is a point in the original domain.</li><li><code>lo</code>: A <code>d</code>-element vector of lower bounds for the original domain.</li><li><code>hi</code>: A <code>d</code>-element vector of upper bounds for the original domain.</li></ul><p><strong>Returns</strong></p><ul><li>The scaled <code>n x d</code> matrix where all values are in <code>[-1, 1]</code>.</li></ul><p><strong>Examples</strong></p><p>```julia-repl julia&gt; lo = [0.0, 10.0]; julia&gt; hi = [1.0, 20.0]; julia&gt; X = [0.5 15.0; 0.0 10.0]; julia&gt; rescale(X, lo, hi) 2×2 Matrix{Float64}:  0.0  0.0 -1.0 -1.0</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/bayesoptfunctions.jl#L97-L119">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="BOOP.upper_confidence_bound-Tuple{Any, Any}" href="#BOOP.upper_confidence_bound-Tuple{Any, Any}"><code>BOOP.upper_confidence_bound</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">  upper_confidence_bound(gp, xnew; κ = 2.0)</code></pre><p>Computes the upper confidence bound criteria at the new point <code>xnew</code> given a Gaussian process (<code>gp</code>) object, and the exploitation/exploitation parameter <code>κ</code>. High values of κ encourage exploration.</p><p>Returns the upper confidence bound criteria at <code>xnew</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Set up GP model.
julia&gt; X_train = [1.0, 2.5, 4.0]; julia&gt; y_train = [sin(x) for x in X_train];
julia&gt; gp_model = GP(X_train&#39;, y_train, MeanZero(), SE(0.0, 0.0));
julia&gt; optimize!(gp_model);
julia&gt; x_candidate = 3.0;

julia&gt; ucb = upper_confidence_bound(gp_model, x_candidate, κ = 2.0)
-0.22727575567547253   # Updete this example, it is from when i used minimum instead of maximum.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/OskarGU/BOOP.jl/blob/884bd4a7d8eac2953005792a03a6463a42646f99/src/acquisitionfunctions.jl#L41-L60">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Saturday 13 September 2025 21:00">Saturday 13 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
