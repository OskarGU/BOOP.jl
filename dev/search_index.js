var documenterSearchIndex = {"docs":
[{"location":"reference/","page":"References","title":"References","text":"continue docs just to see order","category":"page"},{"location":"reference/#Index","page":"References","title":"Index","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"","category":"page"},{"location":"reference/#Ramaining-functions","page":"References","title":"Ramaining functions","text":"","category":"section"},{"location":"reference/#BOOP.BO-NTuple{4, Any}","page":"References","title":"BOOP.BO","text":"BO(f, modelSettings, optimizationSettings, warmStart)\n\nPerforms a full Bayesian Optimization run. ... (resten av docstringen) ...\n\nReturns\n\nA Tuple containing:\ngp: The final, trained GP model.\nX: The full n x d matrix of all evaluated points.\ny: The full n-element vector of all observations.\nobjectMaximizer: The location x of the global maximum of the final posterior mean.\nobjectMaximizerY: The predicted posterior mean value at objectMaximizer. # <– NEW\npostMaxObserved: The location x of the observed point with the highest posterior mean.\npostMaxObservedY: The predicted posterior mean value at postMaxObserved.\n\n...\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.ExpectedMaxGaussian-Tuple{Vector{Float64}, Vector{Float64}}","page":"References","title":"BOOP.ExpectedMaxGaussian","text":"ExpectedMaxGaussian(μ::Vector{Float64}, σ::Vector{Float64})\n\nAnalytically computes the expectation E[max(μ + σZ)] where Z ~ N(0,1).\n\nThis is the core for knowledgeGradientDiscrete. It calculates the expected maximum of a set of correlated Gaussian random variables that share a single source of randomness, Z. The function is robust to cases where slopes (σ) are equal or nearly equal.\n\nArguments\n\nμ::Vector{Float64}: A vector of the current means of the random variables.\nσ::Vector{Float64}: A vector of the sensitivities (\"slopes\") of each random variable with respect to the common random factor Z.\n\nReturns\n\nFloat64: The analytically computed expected maximum value.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.estimate_integral_wsabi-Tuple{Any, Any}","page":"References","title":"BOOP.estimate_integral_wsabi","text":"estimate_integral_wsabi(gp, bounds; n_samples=100_000, y_mean=0.0, y_std=1.0)\n\nEstimates the integral of the original function f(x) using the final GP model trained on warped data g(x) = log(f(x)).\n\nIt uses Monte Carlo integration on the posterior expectation of f(x). E[f(x)] = exp(μg(x) + σ²g(x)/2), where μg and σ²g are the posterior mean and variance of the GP fitted to the log-transformed data.\n\nArguments\n\ngp: The final trained GP model.\nbounds: A tuple (lo, hi) defining the integration domain.\nn_samples: Number of Monte Carlo samples for the approximation.\ny_mean, y_std: The mean and std dev used to standardize the warped y-values,                    needed to un-scale the GP's predictions.\n\nReturns\n\nFloat64: The estimated value of the integral.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.expected_improvement-Tuple{Any, Any, Any}","page":"References","title":"BOOP.expected_improvement","text":"  expected_improvement(gp, xnew, fMax; ξ = 0.01)\n\nComputes the expected improvement given a Gaussian process (gp) object, and the earliest best evaluation (fMax) at the new evaluation point xnew). ξ is a tuning parameter that controls the exploration-exploitation trade-off. A large value of ξ encourages exploration and vice versa.\n\nReturns the expected improvement at xnew.\n\nExamples\n\njulia> Set up GP model.\njulia> X_train = [1.0, 2.5, 4.0]; julia> y_train = [sin(x) for x in X_train];\njulia> gp_model = GP(X_train', y_train, MeanZero(), SE(0.0, 0.0));\njulia> optimize!(gp_model);\n\njulia> # 2. Define the best observed value and a candidate point\njulia> fMax = maximum(y_train);\njulia> x_candidate = 3.0;\n\njulia> # 3. Compute Expected Improvement\njulia> ei = expected_improvement(gp_model, x_candidate, fMax; ξ=0.01)\n≈ 4.89.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.expected_improvement_boundary-Tuple{Any, Any, Any}","page":"References","title":"BOOP.expected_improvement_boundary","text":"expected_improvement_boundary(gp, xnew, ybest; ξ = 0.10, bounds=nothing)\n\nComputes the Expected Improvement (EI) with an optional penalty for points near the boundary of the search space. VERY EXPERIMENTAL.\n\nThe penalty is a multiplicative weight w = prod(1 - x_norm^2) that smoothly pushes the search away from the edges of the scaled [-1, 1] domain. This can be useful to prevent the optimizer from selecting points at the very edge of the feasible region. The idea comes from practical experience that EI otherwise spent too much time exploring the boundaries.\n\nArguments\n\ngp: The trained Gaussian Process model.\nxnew: The candidate point at which to evaluate the EI.\nybest: The current best observed value.\nξ: The exploration-exploitation trade-off parameter.\nbounds: A tuple (lower, upper) defining the original search space, required for the penalty.\n\nReturns\n\nFloat64: The (potentially penalized) Expected Improvement value.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.first_func-Tuple{Any}","page":"References","title":"BOOP.first_func","text":"    first_func(x)\n\nA first dummy function that returns the log of the square of the input value.\n\njulia> T = 500; ϕ = [sin.(2π*(1:T)/T) -0.1*ones(T)]; θ = zeros(T); σ = 1; μ = 2;\njulia> y = simTvARMA(ϕ, θ, μ, σ);\njulia>  tvPeriodogram(y, 25, 15)\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.inv_rescale-Tuple{Any, Any, Any}","page":"References","title":"BOOP.inv_rescale","text":"inv_rescale(X_scaled, lo, hi)\n\nPerforms the inverse of rescale, scaling an n x d matrix X_scaled from the hypercube [-1, 1]^d back to the original domain.\n\nArguments\n\nX_scaled: An n x d matrix where each row is a point in [-1, 1]^d.\nlo: A d-element vector of lower bounds for the original domain.\nhi: A d-element vector of upper bounds for the original domain.\n\nReturns\n\nThe un-scaled n x d matrix in the original domain.\n\nExamples\n\njulia> lo = [0.0, 10.0];\njulia> hi = [1.0, 20.0];\njulia> X_scaled = [0.0 0.0; -1.0 -1.0];\njulia> inv_rescale(X_scaled, lo, hi)\n2×2 Matrix{Float64}:\n 0.5  15.0\n 0.0  10.0\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.knowledgeGradientDiscrete-Tuple{Any, Any, Any}","page":"References","title":"BOOP.knowledgeGradientDiscrete","text":"knowledgeGradientDiscrete(gp, xnew, domain_points)\n\nComputes the Knowledge Gradient (KG) acquisition function where the future maximum is constrained to occur on a fixed, discrete set of points.\n\nThis function is the analytical engine for the knowledgeGradientHybrid heuristic.\n\nArguments\n\ngp: The trained Gaussian Process model.\nxnew: The candidate point at which to evaluate the KG.\ndomain_points: A d x M matrix of M discrete points in the scaled [-1, 1] space where the future maximum is sought.\n\nReturns\n\nFloat64: The discrete Knowledge Gradient value.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.knowledgeGradientHybrid-Tuple{GaussianProcesses.GPE, Any}","page":"References","title":"BOOP.knowledgeGradientHybrid","text":"knowledgeGradientHybrid(gp, xnew, lower, upper; n_z=5)\n\nCalculates the Hybrid Knowledge Gradient (KGh) acquisition function for a candidate point xnew.\n\nThe Hybrid KG combines the strengths of the Monte-Carlo and Discrete KG methods. It uses a small, deterministic set of n_z fantasy scenarios to identify a high-potential set of future maximizers (X_MC), and then uses the fast, analytical Discrete KG algorithm on that small set.\n\nArguments\n\ngp::GPE: The trained Gaussian Process model.\nxnew: The candidate point at which to evaluate the KG.\nlower, upper: The bounds of the search domain for finding the fantasy maximizers.\nn_z::Int: The number of deterministic Z-samples to use (default is 5, as in the paper).\n\nReturns\n\nFloat64: The positive Hybrid Knowledge Gradient value.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.knowledgeGradientMonteCarlo-Tuple{Any, Any}","page":"References","title":"BOOP.knowledgeGradientMonteCarlo","text":"knowledgeGradientMonteCarlo(gp, xnew, lower, upper; n_samples=20)\n\nCalculates the Knowledge Gradient (KG) acquisition function for a candidate point xnew using Monte Carlo. This version gives a noisy surface to the optimization landscape due to MC variation. Try the \"quadrature methods below\".\n\nThe Knowledge Gradient quantifies the expected increase in the maximum estimated value after sampling at xnew.\n\nArguments\n\ngp::GPE: The trained Gaussian Process model.\nxnew: The candidate point at which to evaluate the KG.\nn_samples::Int: The number of Monte Carlo samples to use.\n\nReturns\n\nFloat64: The positive Knowledge Gradient value. This function returns the natural KG score, which should be maximized by the optimization loop.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.knowledgeGradientQuadrature-Tuple{GaussianProcesses.GPE, Any}","page":"References","title":"BOOP.knowledgeGradientQuadrature","text":"knowledgeGradientQuadrature(gp, xnew; n_z=20, alpha=0.5, n_starts=15)\n\nComputes the Knowledge Gradient (KG) using a direct and deterministic quadrature approximation of the expectation integral.\n\nThis is faster and smoother to optimize relative to Monte Carlo and more robust than Hybrid methods. It uses a tail-focused quadrature scheme based on a Beta distribution transformation to increase exploration, while compensating with non-uniform weights to maintain an unbiased estimate. The resulting acquisition surface is deterministic (not noisy) but not necessarily smooth.\n\nArguments\n\ngp: The trained Gaussian Process model.\nxnew: The candidate point at which to evaluate the KG.\nn_z::Int: The number of nodes (fantasy scenarios) used for the quadrature approximation.\nalpha::Float64: Controls the tail-focus of the quadrature nodes. alpha < 1.0 emphasizes the tails (more exploration), while alpha = 1.0 reverts to uniform spacing (on a cdf).\nn_starts::Int: The number of restarts for the inner optimization that finds the maximum of each fantasy posterior.\n\nReturns\n\nFloat64: The Knowledge Gradient value, guaranteed to be non-negative.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.multi_start_maximize-Tuple{Any, Any, Any}","page":"References","title":"BOOP.multi_start_maximize","text":"multi_start_maximize(f, lower, upper; n_starts=20)\n\nFinds the global maximum of a function f within a given box [lower, upper] by using multiple starting points. This is a wrapper around multi_start_minimize that simply minimizes the negative of the function.\n\nArguments\n\nf: The function to maximize.\nlower: A vector of lower bounds.\nupper: A vector of upper bounds.\nn_starts::Int: The number of distinct starting points to use.\n\nReturns\n\nTuple{Float64, Vector{Float64}}: A tuple (best_max, best_argmax) containing the maximum value found and the location where it was found.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.multi_start_minimize-Tuple{Any, Any, Any}","page":"References","title":"BOOP.multi_start_minimize","text":"multi_start_minimize(f, lower, upper; n_starts=20)\n\nFinds the global minimum of a function f within a given box [lower, upper] by using multiple starting points.\n\nThis function uses the L-BFGS optimizer from Optim.jl together with autodiff, starting from n_starts evenly spaced points within the domain to increase the probability of finding a global, rather than local, minimum.\n\nArguments\n\nf: The function to minimize.\nlower: A vector of lower bounds.\nupper: A vector of upper bounds.\nn_starts::Int: The number of distinct starting points to use.\n\nReturns\n\nTuple{Float64, Vector{Float64}}: A tuple (best_min, best_argmin) containing the minimum value found and the location where it was found.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.posteriorMax-Tuple{Any}","page":"References","title":"BOOP.posteriorMax","text":"posteriorMax(gp; n_starts=20)\n\nFinds the global maximum of the GP's posterior mean over the entire continuous, scaled domain [-1, 1]^d using multi-start optimization. This maximum can occur at a previously unobserved point.\n\nArguments\n\ngp: The trained Gaussian Process model.\nn_starts::Int: The number of restarts for the optimization.\n\nReturns\n\nNamedTuple{(:fX_max, :X_max)}: A named tuple containing the maximum value of the posterior mean (fX_max) and its location (X_max) in the scaled space.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.posteriorMaxObs-Tuple{Any, Any}","page":"References","title":"BOOP.posteriorMaxObs","text":"posteriorMaxObs(gp, X_scaled)\n\nFinds the maximum of the GP's posterior mean evaluated only at the points that have already been observed.\n\nArguments\n\ngp: The trained Gaussian Process model.\nX_scaled: A d x n matrix of the n locations already observed, in the scaled space.\n\nReturns\n\nFloat64: The maximum value of the posterior mean among the observed points.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.posterior_cov-Tuple{GaussianProcesses.GPE, AbstractMatrix, AbstractMatrix}","page":"References","title":"BOOP.posterior_cov","text":"posterior_cov(gp::GPE, X1::AbstractMatrix, X2::AbstractMatrix)\n\nComputes the posterior covariance matrix k_n(X1, X2) between two sets of points X1 and X2, given the GP's training data.\n\nThe calculation uses the formula k(X1, X2) - k(X1, X) * K_inv * k(X, X2), using the pre-computed Cholesky factorization of the GP's kernel matrix for high efficiency.\n\nArguments\n\ngp::GPE: The trained Gaussian Process model.\nX1::AbstractMatrix: A d x n1 matrix of n1 points.\nX2::AbstractMatrix: A d x n2 matrix of n2 points.\n\nReturns\n\nMatrix{Float64}: The n1 x n2 posterior covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.posterior_variance-Tuple{Any, Any}","page":"References","title":"BOOP.posterior_variance","text":"posterior_variance(gp, xnew)\n\nComputes the posterior variance of the Gaussian Process gp at a new point xnew.\n\nThis acquisition function directs sampling to regions of highest uncertainty in the model. It is the primary acquisition function for standard Bayesian Quadrature methods like WSABI, but can also be used for pure exploration in Bayesian Optimization.\n\nArguments\n\ngp: The trained Gaussian Process model.\nxnew: The candidate point at which to evaluate the posterior variance.\n\nReturns\n\nFloat64: The posterior variance at xnew.\n\nExamples\n\n```julia-repl julia> Xtrain = [1.0, 5.0]; julia> ytrain = [sin(x) for x in Xtrain]; julia> gp = GP(Xtrain', y_train, MeanZero(), SE(0.0, 0.0)); julia> optimize!(gp);\n\njulia> # The point of highest uncertainty is halfway between the samples julia> xcandidate = 3.0; julia> posteriorvariance(gp, x_candidate) 1.0000010000000002\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.propose_next-Tuple{Any, Any}","page":"References","title":"BOOP.propose_next","text":"propose_next(gp::GPE, f_max; n_restarts::Int, acq_config::AcquisitionConfig)\n\nOptimizes the acquisition function to find the best next point to sample. The optimization is performed in the scaled [-1, 1]^d space.\n\nArguments\n\ngp::GPE: The current trained Gaussian Process model.\nf_max: The current best observed value (or posterior mean max), used by some acquisition functions like EI.\nn_restarts::Int: The number of restarts for the multi-start optimization of the acquisition function.\nacq_config::AcquisitionConfig: A struct holding the configuration for the chosen acquisition function (e.g., EIConfig, UCBConfig, KGQConfig).\n\nReturns\n\nVector{Float64}: The coordinates of the next best point to sample, in the scaled [-1, 1]^d space.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.rescale-Tuple{Any, Any, Any}","page":"References","title":"BOOP.rescale","text":"rescale(X, lo, hi)\n\nScales an n x d matrix X from an original domain to the hypercube [-1, 1]^d.\n\nArguments\n\nX: An n x d matrix where each row is a point in the original domain.\nlo: A d-element vector of lower bounds for the original domain.\nhi: A d-element vector of upper bounds for the original domain.\n\nReturns\n\nThe scaled n x d matrix where all values are in [-1, 1].\n\nExamples\n\n```julia-repl julia> lo = [0.0, 10.0]; julia> hi = [1.0, 20.0]; julia> X = [0.5 15.0; 0.0 10.0]; julia> rescale(X, lo, hi) 2×2 Matrix{Float64}:  0.0  0.0 -1.0 -1.0\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.upper_confidence_bound-Tuple{Any, Any}","page":"References","title":"BOOP.upper_confidence_bound","text":"  upper_confidence_bound(gp, xnew; κ = 2.0)\n\nComputes the upper confidence bound criteria at the new point xnew given a Gaussian process (gp) object, and the exploitation/exploitation parameter κ. High values of κ encourage exploration.\n\nReturns the upper confidence bound criteria at xnew.\n\nExamples\n\njulia> Set up GP model.\njulia> X_train = [1.0, 2.5, 4.0]; julia> y_train = [sin(x) for x in X_train];\njulia> gp_model = GP(X_train', y_train, MeanZero(), SE(0.0, 0.0));\njulia> optimize!(gp_model);\njulia> x_candidate = 3.0;\n\njulia> ucb = upper_confidence_bound(gp_model, x_candidate, κ = 2.0)\n-0.22727575567547253   # Updete this example, it is from when i used minimum instead of maximum.\n\n\n\n\n\n","category":"method"},{"location":"#BOOP","page":"Home","title":"BOOP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for BOOP.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is a get starting message to see that every thing works and we can add stuff.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Some more info","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
