var documenterSearchIndex = {"docs":
[{"location":"reference/","page":"References","title":"References","text":"continue docs just to see order","category":"page"},{"location":"reference/#Index","page":"References","title":"Index","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"","category":"page"},{"location":"reference/#Ramaining-functions","page":"References","title":"Ramaining functions","text":"","category":"section"},{"location":"reference/#BOOP.ExpectedMaxGaussian-Tuple{Vector{Float64}, Vector{Float64}}","page":"References","title":"BOOP.ExpectedMaxGaussian","text":"Calculates E[max(μ + σZ)] where Z ~ N(0,1). This is the stable, core algorithm.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.expected_improvement-Tuple{Any, Any, Any}","page":"References","title":"BOOP.expected_improvement","text":"  expected_improvement(gp, xnew, fMax; ξ = 0.01)\n\nComputes the expected improvement given a Gaussian process (gp) object, and the earliest best evaluation (fMax) at the new evaluation point xnew). ξ is a tuning parameter that controls the exploration-exploitation trade-off. A large value of ξ encourages exploration and vice versa.\n\nReturns the expected improvement at xnew.\n\nExamples\n\njulia> Set up GP model.\njulia> X_train = [1.0, 2.5, 4.0]; julia> y_train = [sin(x) for x in X_train];\njulia> gp_model = GP(X_train', y_train, MeanZero(), SE(0.0, 0.0));\njulia> optimize!(gp_model);\n\njulia> # 2. Define the best observed value and a candidate point\njulia> fMax = maximum(y_train);\njulia> x_candidate = 3.0;\n\njulia> # 3. Compute Expected Improvement\njulia> ei = expected_improvement(gp_model, x_candidate, fMax; ξ=0.01)\n≈ 4.89.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.first_func-Tuple{Any}","page":"References","title":"BOOP.first_func","text":"    first_func(x)\n\nA first dummy function that returns the log of the square of the input value.\n\njulia> T = 500; ϕ = [sin.(2π*(1:T)/T) -0.1*ones(T)]; θ = zeros(T); σ = 1; μ = 2;\njulia> y = simTvARMA(ϕ, θ, μ, σ);\njulia>  tvPeriodogram(y, 25, 15)\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.knowledgeGradientDiscrete-Tuple{GaussianProcesses.GPE, Any, Matrix{Float64}}","page":"References","title":"BOOP.knowledgeGradientDiscrete","text":"Computes the Knowledge Gradient acquisition function for a multi-dimensional GP.\n\nThis version correctly handles multi-dimensional inputs by expecting domain_points to be a D x d matrix.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.knowledgeGradientMonteCarlo-Tuple{GaussianProcesses.GPE, Any}","page":"References","title":"BOOP.knowledgeGradientMonteCarlo","text":"knowledgeGradientMonteCarlo(gp, xnew, lower, upper; n_samples=20)\n\nCalculates the Knowledge Gradient (KG) acquisition function for a candidate point xnew. This version is designed for MAXIMIZATION problems.\n\nThe Knowledge Gradient quantifies the expected increase in the maximum estimated value after sampling at xnew. This implementation uses Monte Carlo simulation.\n\nArguments\n\ngp::GPE: The trained Gaussian Process model.\nxnew: The candidate point at which to evaluate the KG.\nlower, upper: The bounds of the search domain for finding the posterior maximum.\nn_samples::Int: The number of Monte Carlo samples to use.\n\nReturns\n\nFloat64: The positive Knowledge Gradient value. This function returns the natural KG score, which should be maximized by the optimization loop.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.multi_start_maximize-Tuple{Any, Any, Any}","page":"References","title":"BOOP.multi_start_maximize","text":"Performs multi-start maximization and returns both the maximum value and the location (argmax) where it was found.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.multi_start_minimize-Tuple{Any, Any, Any}","page":"References","title":"BOOP.multi_start_minimize","text":"Performs multi-start minimization and returns both the minimum value and the location (argmin) where it was found.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BOOP.upper_confidence_bound-Tuple{Any, Any}","page":"References","title":"BOOP.upper_confidence_bound","text":"  upper_confidence_bound(gp, xnew; κ = 2.0)\n\nComputes the upper confidence bound criteria at the new point xnew given a Gaussian process (gp) object, and the exploitation/exploitation parameter κ. High values of κ encourage exploration.\n\nReturns the upper confidence bound criteria at xnew.\n\nExamples\n\njulia> Set up GP model.\njulia> X_train = [1.0, 2.5, 4.0]; julia> y_train = [sin(x) for x in X_train];\njulia> gp_model = GP(X_train', y_train, MeanZero(), SE(0.0, 0.0));\njulia> optimize!(gp_model);\njulia> x_candidate = 3.0;\n\njulia> ucb = upper_confidence_bound(gp_model, x_candidate, κ = 2.0)\n-0.22727575567547253   # Updete this example, it is from when i used minimum instead of maximum.\n\n\n\n\n\n","category":"method"},{"location":"#BOOP","page":"Home","title":"BOOP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for BOOP.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is a get starting message to see that every thing works and we can add stuff.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
